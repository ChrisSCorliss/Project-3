{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Me \n",
    "## Reddit Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were assigned to pick 2 sub reddits from a website called Reddit. A subreddit is a group or channel of individuals chatting and sharing ideas or opinions centered around a single focus subject. The goal was to pull information from the site. Using whatever methods we wanted. I used api's. Api's are already packaged blocks of information of the web page without all of the html coding. After gathering all desired information. The next step is to clean the data. While cleaning I tried to let the models do most of the heavy lifting but chose to replace some of the information with blank spaces. It was a method to ensue that I grabbed everything I wanted from spots that may have been erased from the models cleaning hyperparameters. Once the data was to more of my liking I plugged the info into Pipelines. A pipeline helps to use multiple models with different uses to stack on eachother. I found that the model combination that worked best was Count Vectorizer and Logistic Regression. I tried Tfidf Vectorizer and 2 naive bayes models, but none of them were able to be more accurate than the first combo I mentioned. While getting mediocre results on the ability to accuratley indentify which sub reddit was which. i felt the under lying factor in all of this was the need for more similar topics to be modeled. Because Data Science is a shared space of Computer Science, they shared alot of similar language which makes it hard to find exactly which was which. If given more time that would have been the first thing I would have done and I would have brought in a stats and analyst subreddit to try and find more key factors in indentifying sub reddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name   |Code   |Description   |\n",
    "|---|---|---|\n",
    "|Count Vectorizer|from sklearn.feature_extraction.text import CountVectorizer | Model used to tokenize and count the amount of times a single word is used across all given data|\n",
    "|Logistic Regression|from sklearn.linear_model import LogisticRegression| Model used to find the relationship of a single point based on the effect of the features given to the algorithm|\n",
    "|train test split|from sklearn.model_selection import train_test_split| Used to seperate the data into sudo gathered data and live feed data|\n",
    "|Gride Search|from sklearn.model_selection import GridSearch| Used to find the best hyperparameters for a given set of models|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just the tools I used to help me find the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "It is possible to correctly indentify which subreddit any given comment under a submission could be with the proper use of modeling cleaning and fully understanding the task at hand. With more knowledge in all aspects, it will  greatly increase your chance to build the most accurate model, and have success in the project. I would suggest to anyone comparing similar sub reddits to pull as many similar as possible to gain the most from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
